\section{Proposed Verification Tool Flow}
%
\begin{figure}[t]
\centering
\vspace*{0.3cm}
\scalebox{.55}{\import{figures/}{new_flow.pspdftex}}
\caption{Tool flow for hardware property verification\label{fig:toolflow}}
\end{figure}
%
%===============================================================================
%\para{Software Verifiers at the heart of Hardware Verification}
%===============================================================================
%
Figure~\ref{fig:toolflow} shows the tool flow for hardware RTL 
verification at various levels -- 
\emph{bit-level}, \emph{word-level}, and \emph{software netlist-level}.  
The bottom flow of figure~\ref{fig:toolflow} shows the bit-level 
verification flow using \ABC.
\ABC does not support Verilog, so we use an open-source synthesis tool,
\yosys\footnote{http://www.clifford.at/yosys/} to translate Verilog
RTL to BLIF or AIGER, which is then passed to \ABC for verification. 
Given a design in Verilog RTL, \yosys bit-blasts it into a bit-level netlist 
along with the property and the resultant netlist is expressed in And-Inverter 
Graph.  The AIG is represented in AIGER format which is one of the standard 
and most prevelant format used by majority of verification tools.


Whereas, the middle flow of figure~\ref{fig:toolflow} shows the 
word-level verification using the tool 
\ebmc~\footnote{\scriptsize{www.cprover.org/hardware/ebmc}}. 
\ebmc supports IEEE 1364.1 Verilog 2005, and synthesize it to a bit-level 
netlist represented in AIGER or a word-level netlist represented in 
SMT-LIB2 format.  The top flow of figure~\ref{fig:toolflow} shows a
verification flow that synthesize a software netlist from RTL using 
the tool \emph{v2c}.  A wide range of representative software 
verification techniques are applied to determine the safety of the
software netlist models. In particular, we use $k$-induction~\cite{SSS00}
(implemented in the tools CBMC~\cite{cbmc.tacas:2004} and
2LS~\cite{kiki}), interpolation
(CPAChecker~\cite{DBLP:conf/cav/BeyerK11}, IMPARA~\cite{impara}
implementing the IMPACT algorithm~\cite{ken}), abstract interpretation
(Ast{\'r}ee~\cite{DBLP:conf/esop/CousotCFMMMR05}), IC3/PDR
(SeaHorn~\cite{DBLP:conf/cav/GurfinkelKKN15}) and automata-based 
trace abstraction (UltimateAutomizer~\cite{DBLP:conf/tacas/HeizmannDGLMSP16}).
%
%===============================================================================
\section{Experimental Results}
%===============================================================================
In this section, we report experimental results for \emph{unbounded} safety 
verification of hardware RTL.  Our experimental contributions are two folds.
%
\begin{enumerate}
  \item Compare off-the-shelf formal verification tools for RTL verification at 
    \emph{bit-level}, \emph{word-level}, and \emph{software netlist level}.  
    To this end, we compare state-of-the-art hardware model checking tools, such as 
    \emph{ABC 1.01} (winner of HWMCC'15) and 
    \ebmcv~\footnote{\scriptsize{www.cprover.org/hardware/ebmc}}, 
    with various software analyzers from SV-COMP 2016, such as 
    \emph{UltimateAutomizer 3292eade} (winner of SV-COMP'16), 
    \emph{CPAChecker 1.4}, 
    \emph{SeaHorn (revision 07666c810d)}, \emph{2LS 0.3.4}, 
    and a commercial abstract 
    interpretation based tool, \emph{Astr{\'e}e}.  
  
 \item  Compare various unbounded verification engines such as $k$-induction, 
    Interpolation, IC3/PDR and Abstraction Interpretation that are employed by 
    verification tools from hardware and software domains.
\end{enumerate}
%
Our experiments were performed on an Intel Xeon machine running at
3.07\,GHz.  We restricted the resources to 5 hours and 32\,GB RAM per
benchmark.  All our benchmarks in Verilog, software netlist models in 
ANSI-C, scripts for running \yosys, \ABC, \ebmcv, and other software 
verification tools are uploaded to a publicly accessible archive website.
\footnote{\scriptsize{http://www.cprover.org/hardware/tcad/}}
%
%===============================================================================
\para{Benchmarks}
%===============================================================================
%
We verified a total of 35 circuits given in Verilog RTL.  Out of 35, 27 
are \emph{safe} benchmarks and 8 are \emph{unsafe}.  The benchmarks in 
our paper are derived from real world hardware benchmark suites, including 
VIS Verilog models, the Texas-97 Benchmark suite, and opencores.org. We synthesize 
each benchmarks into three different netlist formats -- AIGER, SMT-LIB2, ANSI-C.
We classify our benchmarks into two different classes -- data-path intensive
circuits, including Huffman encoder/decoder and a Digital Audio
Input-Output chip (DAIO); and control-intensive designs,
including a non-pipelined 3-stage processor, a Read-Copy-update
mutual exclusion protocol, a FIFO controller, a buffer allocation model,
and an instruction queue controller.   
%
%-------------------------------------------------------------------------------
\para{Properties}
%-------------------------------------------------------------------------------
%
The safety properties are specified as System Verilog assertions (SVA).
The properties are instrumented as assertions in the software netlist. 
Our tool support fragment of SVA properties. Below, we present
an example of property monitor in software netlist corresponding 
to the concurrent assertions in SVA.  Let us consider verification 
of the following properties of Vending machine and Huffman 
encoder/decoder design. Figure~\ref{figure:prop1} and 
Figure~\ref{figure:prop2} shows the SVA property in the left and 
the corresponding monitor for the software netlist in the right. \\
$Assert\_1$: {\em The balance is never negative and never reaches 15.} 
\vspace{-3mm}
\begin{figure}[htbp]
\scriptsize
\begin{tabular}{l|l}
\hline
SVA & Monitor (in C)
\\
\hline
\begin{lstlisting}[mathescape=true,language=Verilog]
p1: assert property 
 (@(posedge clk) 
 vending.total[3]==0 && 
 !(vending.total[4:0]==15)); 
\end{lstlisting}
&
\begin{lstlisting}[mathescape=true,language=C]
int monitor_p1() {
 assert((((vending.total 
 >> 3) & 0x1) == 0) 
 && !(vending.total 
 & 0x1F == 15));
 return 1;
}
\end{lstlisting} \\
\hline
\end{tabular}
\caption{Modeling concurrent assertions in SVA as monitor in software netlist}
\label{figure:prop1}
\end{figure}
%

$Assert\_2$: {\em When a new transmission begins, the decoder is ready in the next clock.} 
\begin{figure}[htbp]
\scriptsize
\begin{tabular}{l|l}
\hline
SVA & Monitor (in C)
\\
\hline
\begin{lstlisting}[mathescape=true,language=Verilog]
p2: assert property 
 (@(posedge clk) 
 encoder.shiftreg[9:1] 
 == 1 |-> ##1 
 decoder.leaf == 1);
\end{lstlisting}
&
\begin{lstlisting}[mathescape=true,language=C]
int monitor_p2() {
 if(((encoder.shiftreg >> 1) 
 & 0x1FF) == 1) {
  check_consequent = 1;
  // call to top level 
  // module of design
  huffman(clk,addr);  
  if(check_consequent == 1) 
    assert(decoder.leaf == 1);
 }
}
\end{lstlisting} \\
\hline
\end{tabular}
\caption{Modeling temporal properties in SVA as monitor in software netlist}
\label{figure:prop2}
\end{figure}
%
%-------------------------------------------------------------------------------
\para{Discussion}
%-------------------------------------------------------------------------------
We classify the analysis results into two categories -- 1) \emph{precise} tools 
that do not use any abstraction and performs precise reasoning using SAT/SMT solvers 
(in section~\ref{precise}), and \emph{abstraction} based tools that performs 
approximate analysis without using SAT/SMT solvers (in section~\ref{abstraction}). 
%
\subsection{Analysis Using Precise tools}~\label{precise}
%
Figures~\ref{fig:kind}--\ref{fig:hybrid} report the comparison of various unbounded 
verification techniques employed by verification tools at bit-level, word-level, 
and software-level.  The plots in these figures reports the runtimes of~12 
representative circuits which are most difficult to solve out of a total 35 
circuits.  Note that \ABC does not implement word-level verification flow, 
so we perform word-level verification using our in-house tool \ebmc.
Figures~\ref{fig:kind}--\ref{fig:hybrid} reports the best runtimes 
among the bit-level and word-level hardware verification tools.  
%
We categorize the unbounded approaches into three classes:
\begin{compactitem}
\item $k$-induction (Figure~\ref{fig:kind})
\item interpolation (Figure~\ref{fig:impact}), and 
\item PDR together with other hybrid techniques (Figure~\ref{fig:hybrid}).  
\end{compactitem}
By hybrid techniques, we refer to predicate
abstraction as implemented in \emph{CPAChecker} and a combination of
$k$-induction, BMC and abstract interpretation as implemented in
\emph{2LS}~\cite{kiki}.  On the $x$-axis is the analysis time in
seconds and on the $y$-axis we list the benchmarks. The vertical red lines on
the right-hand side of the diagrams show timeouts, out of memory,
inconclusive (unknown) results, errors (crashes), and wrong results
(tool bugs) reported by the tools. The tools can be distinguished 
by the size of the circles as well as by colour. 

\para{Analysis using $k$-induction} For safe benchmarks, the results
for bit-level, word-level verifiers and software verifiers are
comparable when the properties are 1-inductive or 2-step inductive.
However, for complex safety properties, \ABC and other abstraction
based software analyzers either timeout or took a long time to
terminate.  We investigated the reason for higher verification times
for some safe benchmarks, such as the FIFO controller, the RCU, and Buffer 
Allocation.  We observe that the properties are not $k$-inductive for
sufficiently large values of $k$, e.g.\ (k=1000) and thus tools based
on $k$-induction either timeout or took long time to
compute the inductive invariant sufficient to prove the property. For
the unsafe benchmarks, for example DAIO and the traffic light controller, where
the bugs are manifested only at 64 and 65 clock cycles respectively,
the verification times using \ABC and \ebmc's $k$-induction engine 
are comparable to \cbmcv and \textsc{2LS}. Figure~\ref{fig:kind} 
reports the time taken by the $k$-induction engine in 
\ABC, \ebmcv, \cbmcv and \textsc{2LS}.  We did not report
the time for \emph{CPAChecker} since the results suggest that 
its $k$-induction engine is not as mature yet. 

\para{Analysis using Interpolation} Figure~\ref{fig:impact} reports
the time taken by the interpolation engine in \ABC, \textsc{IMPARA}
and \emph{CPAChecker}. \ABC is the fastest in 9 out of 12
designs. However, it timed out on three complex benchmarks, RCU, FIFO
and BufAl, whereas the software interpolation tool, \textsc{IMPARA},
which implements IMPACT algorithm solved three instances out of which
one is the complex FIFO design; yet \textsc{IMPARA} either timed out or
ran out of memory for the remaining designs.  \emph{CPAChecker} solved
5 out of 12 cases.  None of the interpolation engines was able to
prove RCU and BufAl.

\para{Analysis using Hybrid techniques} Figure~\ref{fig:hybrid}
reports the time taken by the IC3/PDR engine in \ABC, \emph{SeaHorn}
and other hybrid techniques as implemented in \emph{CPAChecker} and
\textsc{2LS}. \ABC~is the clear winner here; it is the only tool that
proves the FIFO and BufAl benchmarks safe within the given 5h timeout.
\emph{SeaHorn}'s PDR engine solves half of the benchmarks, but
produces false negatives on the other half due to limited support for
bitvectors. \textsc{2LS} successfully solved 8 benchmarks and times 
out on four benchmarks.  \emph{CPAChecker}'s predicate abstraction reliably
solves 7 benchmarks, but timed out on two benchmarks and reports three
wrong results. Note that none of the tools was able to prove RCU.

The bit-level hardware tools performs better than word-level hardware 
verification for our benchmarks.  So, we only report the bit-level results 
obtained using \ABC in Figures~\ref{fig:kind}--\ref{fig:hybrid}.


%We do not report the results using Astr{\'e}e since it requires manual
%directives for data and control partitioning to avoid imprecision;
%nonetheless it generates many false alarms for safe benchmarks.

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
%\scalebox{0.9}{
\begin{tikzpicture}[scale=0.9]
\small
\pgfplotstableread{
Benchmark	ABC-kind	EBMC-kind	CBMC-kind	2LS-kind
BufAl	15000	15000	15000	15000
DAIO	59.92	73.62	229.72	54
Dekker	0.17	15000	15000	15000
FIFOs	15000	15000	15000	15000
Heap	0.34	15000	15000	15000
Huffman	0.02	0.01	0.21	0.24
Ibuf	0.01	0.01	0.26	0.3
RCU	15000	15000	15000	15000
TicTacToe	0.01	0.01	0.23	0.25
n-pipe-mp	0.02	0.26	0.27	0.17
traffic-light	1.52	32.67	29.46	7.98
Vending	0.02	0.02	0.21	0.25
}\datatable

\begin{axis}[
    xbar, xmode=log,
    xmin=0,         
    ytick=data,    
    yticklabels from table={\datatable}{Benchmark},  
    legend style={at={(0.9,1.3)}},
]
\addplot [mark size=5pt,only marks, fill=yellow] table [x={ABC-kind}, y expr=\coordindex] {\datatable};    
\addplot [mark size=4pt,only marks, fill=green!70!blue]table [x={EBMC-kind}, y expr=\coordindex] {\datatable};
\addplot [mark size=3pt,only marks, fill=red!80!yellow] table [x={CBMC-kind}, y expr=\coordindex] {\datatable};
\addplot [mark size=2pt,only marks, fill=blue] table [x={2LS-kind}, y expr=\coordindex] {\datatable};
\addplot [red,sharp plot] coordinates{(15000,0) (15000,11)}
          node [left,rotate=90] at (axis cs:9000,10) {timeout};
\legend{{ABC-kind},{EBMC-kind},{CBMC-kind},{2LS-kind}}
\end{axis}
\end{tikzpicture}
%}
\caption{\label{fig:kind}
Comparison of $k$-induction tools
}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
%\scalebox{0.9}{
\begin{tikzpicture}[scale=0.9]
\small
\pgfplotstableread{
Benchmark	ABC-interpolation	CPA-interpolation	IMPARA-interpolation
BufAl	15000	60000	15000
DAIO	65.74	60000	240000
Dekker	3.73	240000	30000
FIFOs	15000	240000	0.39
Heap	1215.79	60000	15000
Huffman	0.01	3.87	0.41
Ibuf	0.01	4.04	30000
RCU	15000	60000	15000
TicTacToe	0.04	3.3	7870
non-pipe-mp	0.01	3.44	120000
traffic-light	5.79	240000	15000
Vending	0.01	3.65	120000
}\datatable

\begin{axis}[
    xbar, xmode=log,
    xmin=0,         
    ytick=data,    
    yticklabels from table={\datatable}{Benchmark},  
    legend style={at={(0.9,1.2)}},
]
\addplot [mark size=4pt,only marks, fill=yellow] table [x={ABC-interpolation}, y expr=\coordindex] {\datatable};    
\addplot [mark size=3pt,only marks, fill=orange]table [x={CPA-interpolation}, y expr=\coordindex] {\datatable};
\addplot [mark size=2pt,only marks, fill=gray] table [x={IMPARA-interpolation}, y expr=\coordindex] {\datatable};
\addplot [red,sharp plot] coordinates{(15000,0) (15000,11)}
          node [left,rotate=90] at (axis cs:9000,11) {timeout};
\addplot [red,sharp plot] coordinates{(30000,0) (30000,11)}
          node [left,rotate=90] at (axis cs:19000,11) {memout};
\addplot [red,sharp plot] coordinates{(60000,0) (60000,11)}
          node [left,rotate=90] at (axis cs:40000,11) {unknown};
\addplot [red,sharp plot] coordinates{(120000,0) (120000,11)}
          node [left,rotate=90] at (axis cs:80000,11) {error};
\addplot [red,sharp plot] coordinates{(240000,0) (240000,11)}
          node [left,rotate=90] at (axis cs:180000,11) {wrong};
\legend{{ABC-interpolation},{CPA-interpolation},{IMPARA}}
\end{axis}
\end{tikzpicture}
%}
\caption{\label{fig:impact}
Comparison of interpolation-based tools
}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
%\scalebox{0.9}{
\begin{tikzpicture}[scale=0.9]
\small
\pgfplotstableread{
Benchmark	ABC-pdr	SeaHorn-pdr	CPA-predabs	2LS-kiki
BufAl	12250.5	240000	240000	15000
DAIO	0.03	2.16	3.95	55.72
Dekker	0.03	4.83	240000	15000
FIFOs	3759.75	240000	15000	15000
Heap	0.75	240000	10.09	0.91
Huffman	0.02	0.09	4.01	0.25
Ibuf	0.01	240000	3.95	0.73
RCU	15000	240000	15000	15000
TicTacToe	0.03	240000	3.29	0.21
non-pipe-mp	0.01	0.06	3.46	0.20
traffic-light	0.13	1.44	240000	7.48
Vending	0.02	1.28	3.77	0.34
}\datatable

\begin{axis}[
    xbar, xmode=log,
    xmin=0,         
    ytick=data,    
    yticklabels from table={\datatable}{Benchmark},  
    legend style={at={(0.9,1.3)}},
]
\addplot [mark size=5pt,only marks, fill=yellow] table [x={ABC-pdr}, y expr=\coordindex] {\datatable};    
\addplot [mark size=4pt,only marks, fill=green!70!blue]table [x={SeaHorn-pdr}, y expr=\coordindex] {\datatable};
\addplot [mark size=3pt,only marks, fill=orange] table [x={CPA-predabs}, y expr=\coordindex] {\datatable};
\addplot [mark size=2pt,only marks, fill=blue] table [x={2LS-kiki}, y expr=\coordindex] {\datatable};
\addplot [red,sharp plot] coordinates{(15000,0) (15000,11)}
          node [left,rotate=90] at (axis cs:9000,11) {timeout};
\addplot [red,sharp plot] coordinates{(30000,0) (30000,11)}
          node [left,rotate=90] at (axis cs:19000,11) {memout};
\addplot [red,sharp plot] coordinates{(60000,0) (60000,11)}
          node [left,rotate=90] at (axis cs:40000,11) {unknown};
\addplot [red,sharp plot] coordinates{(120000,0) (120000,11)}
          node [left,rotate=90] at (axis cs:80000,11) {error};
\addplot [red,sharp plot] coordinates{(240000,0) (240000,11)}
          node [left,rotate=90] at (axis cs:180000,11) {wrong};
\legend{{ABC-pdr},{SeaHorn-pdr},{CPA-predabs},{2LS-kiki}}
\end{axis}
\end{tikzpicture}
%}
\caption{\label{fig:hybrid}
Comparison of hybrid techniques
}
\end{figure}
%%%%%%%%%%%%%%%%%%% Plot runtimes %%%%%%%%%%%%%%%%%%%%%%%
\input{plot-runtimes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analysis Using Abstraction based tools}~\label{abstraction}
%
Figure~\ref{fig:runtimes} gives the runtime comparsion between UltimateAutomizer 
and Astr{\'e}e.  Both tools operate on an over-approximate abstraction of the 
concrete program.  

For safe benchmarks that are hard to proof, UltimateAutomizer was able to infer 
the necessary invariants required to prove the property within feasible time 
while Astr{\'e}e either timed out or was too imprecise to prove the property. 
\rmcmt{comparision with ABC, quantitative comparison}

Astr{\'e}e reported a total of 7 false alarm out of 35 benchmarks.  
We investigate the reason for the false alarm and observe 
that the bit manipulating nature of these programs prevent Astr{\'e} from 
inferring the necessary invariants required to prove the property using the 
existing set of abstract domains.  Whereas, UltimateAutomizer timed out in~14 
out of 30 benchmarks. For safe benchmarks that are hard to proof, UltimateAutomizer 
was able to infer the necessary invariants required to prove the property within 
feasible time, while Astr{\'e}e was too imprecise to prove the property. 
Astr{\'e}e performed better than UltimateAutomizer for the unsafe benchmarks.  
Astr{\'e}e reported bugs in all 8 unsafe benchmarks. 


We now discuss the analysis using these abstraction based tools.

\para{Abstract Interpretation for RTL Verification Using Astr{\'e}e}
%
One of the benefits of the proposed tool flow in this paper is that the 
synthesis of RTL to software netlist enables the application of techniques 
such as abstract interpretation. Abstract Interpretation~\cite{Cousot92,CC79} is a theory of 
sound approximation of program semantics based on lattice structures. 
Static analysis using abstract interpretation is widely used to verify 
properties of safety-critical systems. 


Astr{\'e}e~\ref{DBLP:conf/esop/CousotCFMMMR05} is a commercial abstract 
interpretation tool developed by AbsInt~\footnote{https://www.absint.com/}.  
It is primarily used for static analysis of safety-critical softwares 
such as avionics software~\ref{DBLP:journals/corr/abs-cs-0701193}.
Astr{\'e}e employs numeric abstract domains, such as intervals that  
abstract variable values as ranges, as well as relational domains that  
infer linear relationships between variables. These abstractions are  
well-suited for programs performing integer and float arithmetic, but  
less so for bit-level Boolean operators, bit-shifts, and bitfields  
packing several values in words. Thus, for bit manipulating programs 
generated by \textsc{v2c}, Astr{\'e}e uses default abstract domains for 
integer values such as interval domain, integer congruences, integer 
bitfields, and finite sets (of possible values).


Moreover, Astr{\'e}e features BDD-based  
abstract domains that can represent non-convex invariants, but they  
are currently limited to Boolean variables and cannot perform  
bit-blasting on integer variables.  Many inductive invariants required  
by the benchmarks cannot be represented, and so not inferred, using 
current abstractions in Astr{\'e}e.

When Astr{\'e}e encounters an \texttt{assert}, it first checks whether 
there are some program states that do not satisfy the assertion, and 
then continues the analysis with only the states that satisfy the assertion. 
Indeed, it assumes that the states that do not satisfy the assertion cause 
the program to stop.  A message such as ``Definite assertion failure" appears 
in case there are no states satisfying the assertion at all, hence, the 
instructions following the assertion are never executed.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\para{Effect of Trace partitioning for Precise analysis in Astr{\'e}e}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\para{Handling Imprecision in Astr{\'e}e}
%
Given that many abstract domains can only represent convex  
(non-disjunctive) numeric properties, control-flow joins and loop  
widening are a major cause of precision loss.  Astr{\'e}e partially  
alleviates the problem through BDD-based domains and 
trace-partitioning~\cite{DBLP:journals/toplas/RivalM07}
(providing a level of path-sensitivity). 
To prevent scalability 
 issues, 
 these costly techniques are enabled locally, 
 through automatic heuristics 
or user guidance. We ran the analysis using the existing heuristics that 
target automatically-generated software netlist obtained from \textsc{v2c}. 


With trace partitioning, Astr{\'e}e automatically insert 
\texttt{\_\_ASTREE\_partition\_control} directives according to a set of 
heuristics.  Astr{\'e}e does not partition ``everything" since this would 
be too expensive in practise. Such a high precision is also normally not 
required for a runtime error analysis. Astr{\'e}e normally start with exisitng 
partitioning heuristics and manually add partitioning directives in places 
where false alarms occur due to a loss of precision resulting from merging 
data-flow information of several control-flow paths (traces).
There are two manual partitioning strategies in Astr{\'e}e - 
1) control-flow partition and 2) partition over all relevant variable values.


A classic example of control partitioning is to analyze each branch of a 
control-flow path separately so as to prevent joins at the control-flow merge 
point.  An example of variable partitioning is as follows. Consider a variable 
named \texttt{mode} that can assume values between 1 and 5.  A partitioning directive 
\texttt{\_\_ASTREE\_partition\_begin((mode));} tells Astr{\'e}e to keep all 
five traces for all five possible values of mode apart 
until the next merge point (i.e., the end of a function, a loop, a sub-statement 
or a merge directive). This of course requires Astr{\'e}e to know that mode is 
in the range $[1;5]$.


Although one can theoretically get an arbitrary high precision in Astr{\'e}e by 
partitioning such that all relevant (or in the extreme: possible) execution 
paths are considered separately, but that is not feasible in practice considering 
the analysis times. Hence, the trick is to find the ``right" partitioning that 
is as imprecise as possible, but can still prove the property under verification.  
However, this is a challenging task.  Astr{\'e}e does not collect data about 
state-space coverage of variables to help user find a good partitioning.  Hence, 
some amount of domain knowledge is required to try different partitioning 
strategies that might enable the analysis to prove the properties under verification. 
For our experiments, manual partitioning based on data as well as control was sufficient 
to precisely prove the properties. 



\Omit{
There can be various sources of imprecision in abstract 
interpretation.  Few common sources of imprecision may occur due to   
\emph{control-flow join}, \emph{loop widening} or use of \emph{imprecise abstract domain}.  
We analyzed the structure of software netlist models to detect the potential sources of 
imprecision that may occur during the analysis using Astr{\'e}e.  Recall that the software 
netlist model retains the control structure of the input RTL design.  Hence, if the original 
RTL has loops or conditional branches inside a module, a software netlist also preserves the 
similar control structure.   
}




\Omit{Standard Static Analysis is Imprecise, but can we do better than bit
blasting ? 

\para{Idea} Partition the traces so that we can prove correctness for each partition.
\para{Solution} To be effecient, we want partitions that are just precise
enough. 
}
\Omit{Best HW runtime versus Best Software runtimes}

\para{Summary of the results} We investigated the reason for large
number of timeouts, wrong results and errors produced by the software
verifiers.  We observed that software netlists heavily use bit-level 
operations and thus bit-precise reasoning ability is necessary for 
the underlying verification engine. However, bit-level operations 
are less prevalent in conventional software and hence less tested 
in software analysis tools. Also, abstract interpretation based tools 
often use numerical abstractions, which are likely to lose 
important bit-precise information.  As a consequence, 
our results show that running conventional sotware verification 
tools on software netlists exhibits many tool bugs (``wrong'').

The abstraction and invariant inference techniques employed in
software tools such as \emph{CPAChecker} and \emph{2LS} have never
been optimized for hardware analysis. But the results in this paper
show that these tools are within one order of magnitude compared to
hardware model checkers for detecting bugs or proving safety for some
of the software netlist models.  
\rmcmt{
We thus believe that there is scope
here for new tools that implement abstract interpretation using
abstract domains developed specifically for this task, e.g.~by
applying abstract conflict driven learning~\cite{dhk2013-popl}.}

%-------------------------------------------------------------------------------
\para{Limitations of the Result}
%-------------------------------------------------------------------------------
Due to lack or absence of Verilog support in other open-source tools that
participate in the HWMCC competition, like \emph{PdTrav}, \emph{IIMC} and 
\emph{V3}, we could not run these tools on our benchmark suite. 


%===============================================================================
\section{Conclusions}
%===============================================================================
We present an approach to unbounded safety verification for hardware
designs given in Verilog RTL, at the heart of which are software
verifiers.  We present a comparison of unbounded verification
techniques at bit-level, word-level and \emph{software netlist} level.

\Omit{
The range of software verification techniques is vast; this paper can
thus only be an initial step.  We will evaluate further software
verification techniques and their application to hardware property
checking and co-verification workloads as future work.
}
