\section{Introduction}\label{sec:intro}
%
Formal hardware verification is now a well-established design technology~\cite{Seligman:2015:FV}. From modest beginnings in the 1980s, extensive corporate and academic research in hardware verification has gone hand-in-hand with gradual but sustained industrial take-up. Examples of landmark achievements in full correctness include verification of the entire execution clusters of the Core 2 Duo~\cite{Core2}  and Core i7 processors~\cite{i7} and the end-to-end ISA verification of several ARM processors~\cite{ARM}. The most prevalent methodology, assertion-based verification~\cite{Foster:2009:AAB}, is by now well supported by mature software tools from the EDA companies.

The industrial success of formal hardware verification has been advanced by decades of research into specialised data structures and algorithms~\cite{ic3,fmcad2000,ken,biere,STE}, tools~\cite{Seger:2005:IEE,abc,ebmc,vis,cadence,synopsysfv}, and methodology~\cite{MCMILLAN2000279,Aagaard:2000:MLH,uclid,word-term,word-bmc,DBLP:conf/lpar/AndrausLS08}.  The most-used technology relies on back-end analysis by propositional satisfiability (SAT) solvers~\cite{Biere1999} or by modern solvers for satisfiability modulo theories (SMT)~\cite{decision_procedures, DBLP:conf/lpar/AndrausLS08,soc-keating,
DBLP:conf/mtv/SunkariCVM07,DBLP:conf/cav/Bjesse08}. Scientifically inspired and practically fruitful research into hardware verification continues today, with scalability an ever-present challenge, along with many others. 

But the past two decades have also seen an explosion in research into automated formal verfication of software~\cite{dkw2008}, with progress vivdly witnessed by the yearly `SV-COMP' competition~\cite{Beyer2017}.  Verification of industrial-scale software is now an established possibility, and commerial offerings are starting to appear. Some techniques used in software verification, such as interpolation~\cite{Interpolants,Kroening:2011:ISV}, have analogues in hardware verification. Others, such as abstract interpretation~\cite{CousotCousot77,Cousot:1996:AI}, have been largely confined to the software domain.  

In this paper, we explore the potential for hardware verification to leverage past and future progress in automated software verification. We translate a hardware model, articulated in Verilog at register transfer level, into a \emph{software netlist}, an ANSI-C program that faithfully reproduces the hardware in software. This opens the door to experiments with using software verification technologies, arising from contemporary program analysis research, to formal verification of hardware RTL designs. We have two aims: first, to establish baseline experimental data for verification of hardware by translation into software; second, to evaluate the effectiveness of some software verification methods---notably abstract interpretation---that have not yet been investigated for hardware.  \rmcmt{Unlike the conventional SAT/SMT-based hardware verification which bit-blast the design into propositional logic, the technique of abstract interpretation analyzes the design over a given abstract domain by computing a least fixed point of the set of equations derived from the design.  The abstract interpretation approach to formal property verification of hardware designs leverage the expressivity of the underlying numerical abstract domains for representing the relational as well as non-relational invariants required for unbounded proofs as well as for finding bugs.  It is worth noting that this technique does not bit-blast the design, hence it is more scalable than SAT/SMT-based analysis.  However, abstract interpretation typically suffers from imprecision due to the use of non-disjunctive abstract domains~\cite{nd} that cannot precisely represent disjunctions.  In this paper, we mitigate the imprecision of the anslysis by manual trace partitioning, which is explained in Section~\ref{abstraction}. } 

Of course, the idea of expressing RTL designs in software has been  advocated in the past, primarily to enable faster simulation. We mention only~\cite{soc-keating}, which highlights verification-related benefits in the context of SoC design.  But we emphasize that the software models in~\cite{soc-keating} are abstractions of hardware that are usually written manually and disconnected from the `golden' RTL design  from which the chip is ultimately realized.  

By contrast, the software netlist representation is an exact translation of the RTL design which reflects the synthesized hardware, not an abstract simulation model.  This is a deliberate design choice, aiming to produce a faithful and transparent software model that leaves maximum flexibility for software verification proof strategies. The principles behind the translation, which we have implemented in a tool called \textsc{V2C}, are explained in Section~\ref{sec:abstraction}

Using our translation, we have experimentally investigated formal verification of hardware using a range of 
native software analyzers.  For baseline comparison with RTL verification, we experiment with software verification technologies that employ SAT/SMT decision procedures~\cite{DBLP:conf/cav/BeyerK11,2ls,cbmc.tacas:2004,DBLP:conf/tacas/HeizmannDGLMSP16}. To 
probe the frontiers, we evaluate the use of abstraction-based software verification 
techniques---most notably abstract interpretation. To ensure our results are openly accessible and can be independently validated, our experiments compare the performance of verification tools drawn from  the Hardware Model Checking Competition (HWMCC) against software verifiers from the Software Verification Competition (SV-COMP).

Extensive experiments with 34 RTL benchmarks confirm that SAT-based, bit-level hardware model checkers currently outperform bit-level software analyzers.  But our experiments also show that a commercial abstract interpreter, Astr{\'e}e, with manual guidance is particularly effective for finding complex bugs as well as proving unbounded safety of software netlist designs. The performance of Astr{\'e}e is comparable to the bit-level hardware model checker ABC~\cite{abc} and in some cases Astr{\'e}e is faster for finding bugs.  On the other hand,
Astr{\'e}e shows a high degree of imprecision on our benchmarks. We have developed manual methods to handle this by guiding the analysis using various trace partitioning directives~\cite{DBLP:journals/toplas/RivalM07}.

